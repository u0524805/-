{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val spark=SparkSession.builder.\n",
    "    master(\"local[*]\").\n",
    "    appName(\"None\").\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class Datarecord(id:String,label:Double,text:String)\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val data=spark.sparkContext.\n",
    "    textFile(\"./newlabeledTrainData25000.tsv\").map({\n",
    "        x =>\n",
    "        var line=x.split(\"\\t\")\n",
    "        Datarecord(line(0),line(1).toDouble,line(2))\n",
    "    }).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------------+\n",
      "|    id|label|                text|\n",
      "+------+-----+--------------------+\n",
      "|5814_8|  1.0|With all this stu...|\n",
      "+------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val regexTokenizer = new RegexTokenizer().\n",
    "  setInputCol(\"text\").\n",
    "  setOutputCol(\"word\").\n",
    "  setPattern(\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val remover = new StopWordsRemover().\n",
    "  setInputCol(\"word\").\n",
    "  setOutputCol(\"stopWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val tf = new CountVectorizer().\n",
    "      setInputCol(\"stopWord\").\n",
    "      setOutputCol(\"cv_vector\").\n",
    "      setVocabSize(300).  \n",
    "      setMinDF(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val hashingTF = new HashingTF().\n",
    "    setInputCol(\"stopWord\").\n",
    "    setOutputCol(\"htf_vector\").\n",
    "    setNumFeatures(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val idf = new IDF().\n",
    "    setInputCol(hashingTF.getOutputCol).\n",
    "    setOutputCol(\"tfidf_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val layers = Array[Int](300,15,14, 2)\n",
    "val trainer = new MultilayerPerceptronClassifier().\n",
    "  setLayers(layers).\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"cv_vector\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pipeline = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover, tf, hashingTF, idf, trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val Array(training, test) = data.randomSplit(Array(0.8, 0.2))\n",
    "val mlp_model = pipeline.fit(training)\n",
    "val testResult = mlp_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        id|label|                text|                word|            stopWord|           cv_vector|          htf_vector|        tfidf_vector|prediction|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|\"\"10116_1\"|  0.0|\"I honestly want ...|[i, honestly, wan...|[honestly, want, ...|(300,[0,1,3,4,14,...|(500,[27,31,44,55...|(500,[27,31,44,55...|       1.0|\n",
      "|\"\"10205_2\"|  0.0|\"So, I'm wonderin...|[so, i, m, wonder...|[wondering, watch...|(300,[0,1,2,3,5,7...|(500,[3,20,25,31,...|(500,[3,20,25,31,...|       0.0|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testResult.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7499010684606252\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val predictionAndLabels = testResult.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "println(\"Accuracy:\"+evaluator.evaluate(predictionAndLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  tf_idf ---MLP\n",
    "\n",
    "### Layers:300  tf-idf:500\n",
    "\n",
    "### Accuracy:0.749"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rf = new RandomForestClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"tfidf_vector\").\n",
    "  setNumTrees(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pipeline1 = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover, hashingTF, idf, rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val rf_model = pipeline1.fit(training)\n",
    "val rftestResult = rf_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7130985358132172\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val predictionAndLabels1 = rftestResult.select(\"prediction\", \"label\")\n",
    "val evaluator1 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "println(\"Accuracy:\"+evaluator1.evaluate(predictionAndLabels1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest\n",
    "\n",
    "## tree:40 tf-idf:500\n",
    "##  Accuracy:0.713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pca_hashingTF30 = new HashingTF().\n",
    "    setInputCol(\"stopWord\").\n",
    "    setOutputCol(\"htf_vector3\").\n",
    "    setNumFeatures(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pca_idf30 = new IDF().\n",
    "    setInputCol(pca_hashingTF3.getOutputCol).\n",
    "    setOutputCol(\"pca_tfidf_vector3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pca_rf30 = new RandomForestClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"pca_tfidf_vector3\").\n",
    "  setNumTrees(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pipeline30 = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover, pca_hashingTF30, pca_idf30, pca_rf30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val pca_rf_model30 = pipeline30.fit(training)\n",
    "val pca_rftestResult30 = pca_rf_model30.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7063711911357341\n"
     ]
    }
   ],
   "source": [
    "val pca_predictionAndLabels30 = pca_rftestResult30.select(\"prediction\", \"label\")\n",
    "val evaluator30 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "println(\"Accuracy:\"+evaluator30.evaluate(pca_predictionAndLabels30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA-RandomForest\n",
    "\n",
    "# tf-idf:500 tree:30 accuracy:0.706\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "val word2vec5 = new Word2Vec().\n",
    "  setInputCol(\"stopWord\").\n",
    "  setOutputCol(\"w2v_vector5\").\n",
    "  setVectorSize(100).\n",
    "  setMinCount(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val word2vec_rf5 = new RandomForestClassifier().\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"w2v_vector5\").\n",
    "  setNumTrees(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val word2vex_pipeline5 = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover, word2vec5, word2vec_rf5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val word2vex_rf_model5 = word2vex_pipeline5.fit( training )\n",
    "val word2vex_testResult5 = word2vex_rf_model5.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7853185595567868\n"
     ]
    }
   ],
   "source": [
    "val word2vex_predictionAndLabels5 = word2vex_testResult5.select(\"prediction\", \"label\")\n",
    "val evaluator5 = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "println(\"Accuracy:\"+evaluator5.evaluate(word2vex_predictionAndLabels5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec-RF\n",
    "\n",
    "# size:100 tree:30 accuracy:0.785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
