{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.\n",
    "      master(\"local[*]\").\n",
    "      appName(\"spark session example\").\n",
    "      getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "case class DataRecord(id:String,label: Double, text: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//原始資料讀入，轉成DataFrame\n",
    "val data = spark.\n",
    "    sparkContext.\n",
    "    textFile(\"labeledTrainData25000.tsv\").map({\n",
    "    x =>\n",
    "    var line = x.split(\"\\t\")\n",
    "    DataRecord(line(0),line(1).toDouble,line(2))//\n",
    "}).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+\n",
      "|      id|label|                text|\n",
      "+--------+-----+--------------------+\n",
      "|\"5814_8\"|  1.0|\"With all this st...|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//斷詞\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "val regexTokenizer = new RegexTokenizer().\n",
    "  setInputCol(\"text\").\n",
    "  setOutputCol(\"word\").\n",
    "  setPattern(\"(\\\\W|\\\\d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//去除停用詞\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "val remover = new StopWordsRemover().\n",
    "  setInputCol(\"word\").\n",
    "  setOutputCol(\"stopWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//count vector TF向量模型\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "val tf = new CountVectorizer().\n",
    "      setInputCol(\"stopWord\").\n",
    "      setOutputCol(\"cv_vector\").\n",
    "      setVocabSize(200).  // 最多幾個字詞? 取最高頻的/////額外控制/////\n",
    "      setMinDF(1) //統計字詞大於等於1次/////額外控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//CPA維度縮減\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "val pca = new PCA().\n",
    "  setInputCol(\"cv_vector\").\n",
    "  setOutputCol(\"pca_vector\").\n",
    "  setK(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//搭配一個你想要的分類器MPL\n",
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "\n",
    "val layers = Array[Int](5, 75, 50, 2)/////維度為5與上同/////\n",
    "val trainer = new MultilayerPerceptronClassifier().\n",
    "  setLayers(layers).\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"pca_vector\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//用一個管子串接起來\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val pipeline = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover,tf, pca, trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//將資料切成training, test 這裡因為資料太少，假設兩個資料集跟原始的資料集一樣!\n",
    "val Array(training, test) = data.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//把training資料集 丟入管線 \n",
    "val mlp_model = pipeline.fit( training )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "val testResult = mlp_model.transform(  test )\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val predictionAndLabels = testResult.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.5682641965364479\n"
     ]
    }
   ],
   "source": [
    "println(\"Accuracy:\"+evaluator.evaluate(predictionAndLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|        id|label|                text|                word|            stopWord|           cv_vector|          pca_vector|prediction|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     \"0_9\"|  1.0|\"Bromwell High is...|[bromwell, high, ...|[bromwell, high, ...|(200,[3,6,11,13,2...|[-0.3557184500977...|       1.0|\n",
      "| \"10002_1\"|  0.0|\"Sorry everyone,,...|[sorry, everyone,...|[sorry, everyone,...|(200,[2,6,9,10,12...|[-0.8093691670658...|       0.0|\n",
      "| \"10004_8\"|  1.0|\"This isn't the c...|[this, isn, t, th...|[comedic, robin, ...|(200,[0,1,4,9,13,...|[-10.125619279929...|       0.0|\n",
      "| \"10005_3\"|  0.0|\"The second attem...|[the, second, att...|[second, attempt,...|(200,[0,1,2,3,4,8...|[-5.1893160399363...|       0.0|\n",
      "| \"10006_4\"|  0.0|\"I don't know who...|[i, don, t, know,...|[know, blame, tim...|(200,[1,3,7,10,12...|[-0.6743789438206...|       0.0|\n",
      "| \"10007_1\"|  0.0|\"This film is med...|[this, film, is, ...|[film, mediocre, ...|(200,[2,3,38,44,5...|[-0.3055687860277...|       1.0|\n",
      "| \"10011_3\"|  0.0|\"This film is abo...|[this, film, is, ...|[film, male, esco...|(200,[0,2,7,8,13,...|[-6.4251916745027...|       0.0|\n",
      "| \"10013_1\"|  0.0|\"A worn-out plot ...|[a, worn, out, pl...|[worn, plot, man,...|(200,[0,1,2,3,5,8...|[-6.6804193410543...|       1.0|\n",
      "| \"10015_2\"|  0.0|\"Ghost of Dragstr...|[ghost, of, drags...|[ghost, dragstrip...|(200,[1,2,15,34,5...|[-0.4623333738457...|       0.0|\n",
      "| \"10016_4\"|  0.0|\"\\\"Ghost of Drags...|[ghost, of, drags...|[ghost, dragstrip...|(200,[0,1,3,4,12,...|[-2.6121581991730...|       0.0|\n",
      "|  \"1001_8\"|  1.0|\"Somewhat funny a...|[somewhat, funny,...|[somewhat, funny,...|(200,[0,1,2,5,7,1...|[-4.5347253861139...|       1.0|\n",
      "| \"10020_3\"|  0.0|\"Vijay Krishna Ac...|[vijay, krishna, ...|[vijay, krishna, ...|(200,[0,2,3,4,5,6...|[-4.7477057629332...|       1.0|\n",
      "| \"10021_2\"|  0.0|\"The worst movie ...|[the, worst, movi...|[worst, movie, se...|(200,[0,1,4,6,9,2...|[-6.2719898330400...|       0.0|\n",
      "| \"10022_7\"|  1.0|\"Aro Tolbukhin bu...|[aro, tolbukhin, ...|[aro, tolbukhin, ...|(200,[0,1,14,16,4...|[-6.1013075811279...|       0.0|\n",
      "| \"10024_9\"|  1.0|\"There's so many ...|[there, s, so, ma...|[many, things, fa...|(200,[0,1,3,4,7,1...|[-5.5916829354232...|       0.0|\n",
      "| \"10026_7\"|  1.0|\"Without Kirsten ...|[without, kirsten...|[without, kirsten...|(200,[2,8,33,34,3...|[-0.3431039794926...|       1.0|\n",
      "|\"10032_10\"|  1.0|\"I loved this mov...|[i, loved, this, ...|[loved, movie, si...|(200,[0,1,27,49,1...|[-2.1495320530322...|       0.0|\n",
      "| \"10032_4\"|  0.0|\"The storyline wa...|[the, storyline, ...|[storyline, okay,...|(200,[0,1,2,3,5,6...|[-4.9457980215691...|       0.0|\n",
      "| \"10035_9\"|  1.0|\"I think James Ca...|[i, think, james,...|[think, james, ca...|(200,[0,1,2,3,5,9...|[-3.2738665055570...|       0.0|\n",
      "| \"10039_1\"|  0.0|\"This movie is ho...|[this, movie, is,...|[movie, horrible,...|(200,[0,1,2,4,7,1...|[-8.9029074965565...|       0.0|\n",
      "+----------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testResult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#改成完整的資料集davinci_movie_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.\n",
    "      master(\"local[*]\").\n",
    "      appName(\"spark session example\").\n",
    "      getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "case class DataRecord(label: Double, text: String)\n",
    "\n",
    "//原始資料讀入，轉成DataFrame\n",
    "val data = spark.\n",
    "    sparkContext.\n",
    "    textFile(\"./davinci_movie_reviews.txt\").map({\n",
    "    x =>\n",
    "    var line = x.split(\"\\t\")\n",
    "    DataRecord(line(0).toDouble,line(1))\n",
    "}).toDF()\n",
    "/////讀檔/////\n",
    "\n",
    "//斷詞\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "val regexTokenizer = new RegexTokenizer().\n",
    "  setInputCol(\"text\").\n",
    "  setOutputCol(\"word\").\n",
    "  setPattern(\"(\\\\W|\\\\d)\")\n",
    "\n",
    "//去除停用詞\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "val remover = new StopWordsRemover().\n",
    "  setInputCol(\"word\").\n",
    "  setOutputCol(\"stopWord\")\n",
    "\n",
    "//count vector TF向量模型\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "val tf = new CountVectorizer().\n",
    "      setInputCol(\"stopWord\").\n",
    "      setOutputCol(\"cv_vector\").\n",
    "      setVocabSize(6000).  /// 最多幾個字詞? 取最高頻的  字數別太多 記憶體會爆掉!/////額外控制/////最大6000\n",
    "      setMinDF(1) //統計字詞大於等於1次/////額外控制/////\n",
    "\n",
    "//CPA維度縮減\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "val pca = new PCA().\n",
    "  setInputCol(\"cv_vector\").\n",
    "  setOutputCol(\"pca_vector\").\n",
    "  setK(200) /////設定維度為5/////\n",
    "\n",
    "//搭配一個你想要的分類器MPL\n",
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "\n",
    "val layers = Array[Int](200, 60, 50, 2)/////維度為5與上同/////\n",
    "val trainer = new MultilayerPerceptronClassifier().\n",
    "  setLayers(layers).\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"pca_vector\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(500)\n",
    "\n",
    "//用一個管子串接起來\n",
    "import org.apache.spark.ml.Pipeline\n",
    "val pipeline = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover,tf, pca, trainer))\n",
    "\n",
    "//將資料切成training, test 這裡因為資料太少，假設兩個資料集跟原始的資料集一樣!\n",
    "val Array(training, test) = data.randomSplit(Array(0.8, 0.2))/////打開/////\n",
    "\n",
    "//val training = data/////注解掉/////\n",
    "//val test = data/////注解掉/////\n",
    "\n",
    "//把training資料集 丟入管線 \n",
    "val mlp_model = pipeline.fit( training )\n",
    "\n",
    "//預測測試資料集的答案\n",
    "val testResult = mlp_model.transform(  test )\n",
    "\n",
    "\n",
    "//計算準確度\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val predictionAndLabels = testResult.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9916955017301038\n"
     ]
    }
   ],
   "source": [
    "println(\"Accuracy:\"+evaluator.evaluate(predictionAndLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|                text|                word|            stopWord|           cv_vector|          pca_vector|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  1.0|\" I could have di...|[i, could, have, ...|[could, discussed...|(1748,[5,6,7,11,1...|[-0.5518235524981...|       1.0|\n",
      "|  1.0|* He deemed me co...|[he, deemed, me, ...|[deemed, cool, li...|(1748,[0,1,11,95]...|[-0.8196292397423...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testResult.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
