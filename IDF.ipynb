{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.\n",
    "      master(\"local[*]\").\n",
    "      appName(\"spark session example\").\n",
    "      getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "case class DataRecord(id:String,label: Double, text: String)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//原始資料讀入，轉成DataFrame\n",
    "val data = spark.\n",
    "    sparkContext.\n",
    "    textFile(\"labeledTrainData25000.tsv\").map({\n",
    "    x =>\n",
    "    var line = x.split(\"\\t\")\n",
    "    DataRecord(line(0),line(1).toDouble,line(2))//\n",
    "}).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------------+\n",
      "|      id|label|                text|\n",
      "+--------+-----+--------------------+\n",
      "|\"5814_8\"|  1.0|\"With all this st...|\n",
      "+--------+-----+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//斷詞\n",
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "val regexTokenizer = new RegexTokenizer().\n",
    "  setInputCol(\"text\").\n",
    "  setOutputCol(\"word\").\n",
    "  setPattern(\"(\\\\W|\\\\d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//去除停用詞\n",
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "val remover = new StopWordsRemover().\n",
    "  setInputCol(\"word\").\n",
    "  setOutputCol(\"stopWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//CountVectorizer詞頻  若沒有用到 可以省略\n",
    "val tf = new CountVectorizer().\n",
    "      setInputCol(\"stopWord\").\n",
    "      setOutputCol(\"cv_vector\").\n",
    "      setVocabSize(9).  // 最多幾個字詞? 取最高頻的\n",
    "      setMinDF(2) //統計字詞大於等於2次\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//HashingTF詞頻\n",
    "val hashingTF = new HashingTF().\n",
    "    setInputCol(\"stopWord\").\n",
    "    setOutputCol(\"htf_vector\").\n",
    "    setNumFeatures(8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val idf = new IDF().\n",
    "    setInputCol(hashingTF.getOutputCol).\n",
    "    setOutputCol(\"tfidf_vector\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "\n",
    "//搭配一個你想要的分類器MPL\n",
    "//注意這裡選擇的是tfidf_vector\n",
    "//輸入向量模型有三種可以選擇--count vector, hashingTF, htfidf 哪一種最好?要依結果的準確度去選擇!\n",
    "\n",
    "val layers = Array[Int](8, 15, 14, 2)\n",
    "val trainer = new MultilayerPerceptronClassifier().\n",
    "  setLayers(layers).\n",
    "  setLabelCol(\"label\").\n",
    "  setFeaturesCol(\"tfidf_vector\").\n",
    "  setPredictionCol(\"prediction\").\n",
    "  setBlockSize(128).\n",
    "  setSeed(1234L).\n",
    "  setMaxIter(500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//用一個管子串接起來\n",
    "val pipeline = new Pipeline().\n",
    "      setStages(Array(regexTokenizer, remover, tf, hashingTF, idf, trainer))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//將資料切成training, test 這裡因為資料太少，假設兩個資料集跟原始的資料集一樣!\n",
    "\n",
    "val Array(training, test) = data.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//把training資料集 丟入管線 \n",
    "val mlp_model = pipeline.fit( training )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "//預測測試資料集的答案\n",
    "val testResult = mlp_model.transform(  test )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|       id|label|                text|                word|            stopWord|           cv_vector|          htf_vector|        tfidf_vector|prediction|\n",
      "+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    \"0_3\"|  0.0|\"Story of a man w...|[story, of, a, ma...|[story, man, unna...|(9,[5,6,7,8],[1.0...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       1.0|\n",
      "|\"10000_8\"|  1.0|\"Homelessness (or...|[homelessness, or...|[homelessness, ho...|(9,[0,2,3,4],[8.0...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10002_7\"|  1.0|\"This is easily t...|[this, is, easily...|[easily, underrat...|       (9,[2],[2.0])|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10004_3\"|  0.0|\"\\\"It appears tha...|[it, appears, tha...|[appears, many, c...| (9,[5,8],[1.0,1.0])|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10004_8\"|  1.0|\"This isn't the c...|[this, isn, t, th...|[comedic, robin, ...|(9,[0,1,4],[10.0,...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10005_3\"|  0.0|\"The second attem...|[the, second, att...|[second, attempt,...|(9,[0,1,2,3,4,8],...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10005_7\"|  1.0|\"Yes its an art.....|[yes, its, an, ar...|[yes, art, succes...|(9,[0,1,2,5,6,7],...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10006_4\"|  0.0|\"I don't know who...|[i, don, t, know,...|[know, blame, tim...|(9,[1,3,7],[1.0,1...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10008_2\"|  0.0|\"The film is bad....|[the, film, is, b...|[film, bad, way, ...|(9,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       1.0|\n",
      "|\"10010_3\"|  0.0|\"Plot is not wort...|[plot, is, not, w...|[plot, worth, dis...|(9,[0,1,4,5,7],[4...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10010_7\"|  1.0|\"There are many i...|[there, are, many...|[many, illnesses,...|(9,[1,2,3,7],[1.0...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10011_3\"|  0.0|\"This film is abo...|[this, film, is, ...|[film, male, esco...|(9,[0,2,7,8],[6.0...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10011_9\"|  1.0|\"I enjoyed The Ni...|[i, enjoyed, the,...|[enjoyed, night, ...|(9,[0,1,3,5],[10....|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10015_2\"|  0.0|\"Ghost of Dragstr...|[ghost, of, drags...|[ghost, dragstrip...| (9,[1,2],[3.0,1.0])|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       1.0|\n",
      "|\"10015_8\"|  1.0|\"Popular radio st...|[popular, radio, ...|[popular, radio, ...|(9,[0,1,2,3,5,8],...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10017_4\"|  0.0|\"\\\"Ghost of Drags...|[ghost, of, drags...|[ghost, dragstrip...| (9,[1,8],[1.0,1.0])|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10019_3\"|  0.0|\"Kareena Kapoor i...|[kareena, kapoor,...|[kareena, kapoor,...|(9,[0,1,3,4,5,6,7...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "| \"1001_4\"|  0.0|\"I rented this on...|[i, rented, this,...|[rented, dvd, kin...|(9,[1,2,3,7,8],[2...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10022_4\"|  0.0|\"Summer season is...|[summer, season, ...|[summer, season, ...|(9,[0,1,3,4,6,7,8...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       0.0|\n",
      "|\"10024_3\"|  0.0|\"First lesson tha...|[first, lesson, t...|[first, lesson, f...|(9,[0,2,3,5],[6.0...|(8,[0,1,2,3,4,5,6...|(8,[0,1,2,3,4,5,6...|       1.0|\n",
      "+---------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "testResult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.5470912266559184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "//計算準確度\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val predictionAndLabels = testResult.select(\"prediction\", \"label\")\n",
    "val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n",
    "println(\"Accuracy:\"+evaluator.evaluate(predictionAndLabels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
